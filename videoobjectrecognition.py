# -*- coding: utf-8 -*-
"""VideoObjectRecognition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BuBQ7DzWJ9e4PUWHXK7GY_3qpCaRYXZQ

# Faster R-CNN model from scratch

### Setting up the environment for the Faster RCNN implementation:

1. **Data Download**
    - Download the PASCAL VOC 2012 dataset using wget command.
    - Extract the downloaded dataset tarball.

2. **Annotation Parsing Function**
    - `parse_annotations(anno_path)`: This function parses XML annotations of the PASCAL VOC dataset.
        - It extracts the bounding box details (coordinates) and their corresponding class labels for each image.
        - Uses `xml.etree.ElementTree` for parsing XML files.
        - A label to integer mapping (`label_map`) is defined to convert string class labels to integers.
        - The function processes each XML file in the specified directory, extracts the bounding box and label details, and stores them in a dictionary (`img_annotations`). This dictionary maps image filenames (without extension) to a list of bounding box details.
        - If any error occurs during XML parsing, the respective file is skipped.
        
3. **Annotation Extraction**
    - The path to the annotations folder of the PASCAL VOC dataset is defined (`anno_path`).
    - The `parse_annotations` function is then called to extract and store the annotations and label map.
    - An inverse of the label map (`inverse_label_map`) is also created.
    - As an example, the annotations for the image `2012_000051` are printed.
"""

import numpy as np
import os
import math
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import torchvision
import torchvision.transforms as transforms
from torchvision.transforms import functional as F
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from torchvision.datasets import VOCDetection
from torchvision.models.detection import FasterRCNN
from torchvision.models import resnet50
from torchvision.models.detection.rpn import AnchorGenerator
from torchvision.transforms import ToTensor
from torchvision.ops import RoIAlign

!pip install opencv-python
import cv2

from PIL import Image
import xml.etree.ElementTree as ET
from sklearn.model_selection import train_test_split
from numpy.core.fromnumeric import resize

!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
!tar -xf VOCtrainval_11-May-2012.tar

def parse_annotations(anno_path):
    """
    Parses PASCAL VOC annotations and extracts bounding box details and their
    corresponding labels for each image.

    Parameters:
    - anno_path (str): Path to the directory containing the XML annotations.

    Returns:
    - dict: Dictionary mapping image filenames (without extension) to a list
            of bounding box details (dict format).
    """
    img_annotations = {}

    # Define a label to integer mapping
    label_map = {
        'aeroplane': 0, 'bicycle': 1, 'bird': 2, 'boat': 3,
        'bottle': 4, 'bus': 5, 'car': 6, 'cat': 7, 'chair': 8,
        'cow': 9, 'diningtable': 10, 'dog': 11, 'horse': 12,
        'motorbike': 13, 'person': 14, 'pottedplant': 15,
        'sheep': 16, 'sofa': 17, 'train': 18, 'tvmonitor': 19
    }

    for file in os.listdir(anno_path):
        if file.endswith('.xml'):
            file_path = os.path.join(anno_path, file)

            try:
                tree = ET.parse(file_path)
                root = tree.getroot()
            except ET.ParseError:
                print(f"Error parsing {file}")
                continue

            boxes = []
            for obj in root.findall('object'):
                label = obj.find('name').text
                box = obj.find('bndbox')

                # Check if all necessary information is available
                if label is None or box is None:
                    continue

                try:
                    xmin = int(round(float(box.find('xmin').text)))
                    ymin = int(round(float(box.find('ymin').text)))
                    xmax = int(round(float(box.find('xmax').text)))
                    ymax = int(round(float(box.find('ymax').text)))
                except (TypeError, ValueError):
                    # In case of missing or malformed bounding box info
                    continue

                boxes.append({
                    "label": label_map.get(label, -1),  # Use the integer mapping; -1 for unknown labels
                    "xmin": xmin,
                    "ymin": ymin,
                    "xmax": xmax,
                    "ymax": ymax
                })

            if boxes:
                img_annotations[file[:-4]] = boxes

    return img_annotations, label_map

anno_path = 'VOCdevkit/VOC2012/Annotations'
annotations, label_map = parse_annotations(anno_path)
inverse_label_map = {v: k for k, v in label_map.items()}
print(annotations.get('2012_000051'))

"""### Building the Dataset and Dataloader:

1. **VOCDataset Class**
    - Custom dataset class (`VOCDataset`) for handling the PASCAL VOC dataset. This class is derived from PyTorch's Dataset class.
    - `__init__`: Initialize the dataset by specifying the directory of the images (`img_dir`), annotations dictionary, and an optional image transformation (`transform`).
    - `__len__`: Return the total number of images in the dataset.
    - `__getitem__`: Fetch an image-target pair based on an index:
        - Image is read using OpenCV, converted from BGR to RGB.
        - Corresponding bounding boxes and labels are extracted from the annotations.
        - If a transformation is specified, it's applied to the image.
        - Constructs a dictionary (`target`) for each image containing the boxes and labels.
        
2. **collate_fn Function**
    - Defines a collation function (`collate_fn`) to gather multiple samples fetched by the dataloader into batches.
    - This is necessary because the number of bounding boxes per image can vary, so custom batch construction logic is used.

3. **Data Splitting**
    - Parses the PASCAL VOC annotations and separates the dataset into training (80%) and testing (20%) based on image names.

4. **Image Transformations**
    - Defines a transformation pipeline (`transform`) for the images:
        - Convert the image from a numpy array to a PIL Image.
        - Resize the image to dimensions (600, 800).
        - Convert the image to a PyTorch tensor.

5. **Data Loaders**
    - Specify the directory where the actual image files are located (`img_dir`).
    - Create the training dataset and dataloader (`train_dataset` and `train_loader`):
        - The training dataset extracts images and targets based on the training split.
        - DataLoader batches and shuffles the training data, and uses the custom collate function.
    - Similarly, create the testing dataset and dataloader (`test_dataset` and `test_loader`).
"""

# Dataset class
class VOCDataset(Dataset):
    def __init__(self, img_dir, annotations, transform=None):
        self.img_dir = img_dir
        self.img_names = list(annotations.keys())
        self.annotations = annotations
        self.transform = transform

    def __len__(self):
        return len(self.img_names)

    def __getitem__(self, idx):
        img_name = self.img_names[idx]
        img_path = os.path.join(self.img_dir, img_name + '.jpg')
        image = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)

        boxes = []
        labels = []
        for anno in self.annotations[img_name]:
            boxes.append([anno['xmin'], anno['ymin'], anno['xmax'], anno['ymax']])
            labels.append(anno['label'])
        boxes = torch.tensor(boxes, dtype=torch.float32)

        if self.transform:
            image = self.transform(image)

        # Construct target dictionary for each image
        target = {}
        target["boxes"] = boxes
        target["labels"] = torch.tensor(labels, dtype=torch.int64)

        return image, target

def collate_fn(batch):
    images, targets = zip(*batch)
    return list(images), list(targets)

img_annotations, _ = parse_annotations('VOCdevkit/VOC2012/Annotations')
img_names = list(img_annotations.keys())
split = int(0.8 * len(img_names))
X_train = img_names[:split]
X_test = img_names[split:]

# Transformations and Data Loaders
transform = transforms.Compose([
    transforms.ToPILImage(),
    transforms.Resize((600, 800)),
    transforms.ToTensor()
])

img_dir = 'VOCdevkit/VOC2012/JPEGImages'
train_dataset = VOCDataset(img_dir, {name: annotations[name] for name in X_train}, transform=transform)
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
test_dataset = VOCDataset(img_dir, {name: annotations[name] for name in X_test}, transform=transform)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

"""### Building a Custom Backbone, Model Configuration, Training, and Inference:

1. **CustomBackbone Class**
    - A custom CNN backbone designed for feature extraction from images.
    - Four convolutional blocks are defined, each followed by batch normalization.
    - The forward method propagates the input through these layers.

2. **Model Configuration**
    - Instantiates the custom backbone.
    - Sets up an RPN anchor generator with specific sizes and aspect ratios.
    - Initializes a Region of Interest (RoI) pooling layer.
    - Constructs the Faster R-CNN model using the custom backbone, specified RPN, and RoI configurations.

3. **Trainer Class**
    - A utility class for training the Faster R-CNN model, making predictions, and saving the model weights.
    - `__init__`: Prepares the model, optimizer, learning rate scheduler, and other necessary components.
    - `transform_targets`: Helper function to structure target data in the required format.
    - `train`: Train the model for a given number of epochs.
    - `predict`: Use the trained model to make predictions on the test dataset. (Note: This just prints the first prediction for demonstration purposes.)
    - `save_model`: Save the model weights to a specified path.

4. **Training and Evaluation**
    - The model is trained on the given data and then a prediction is made using the test dataset.
    - The trained model's weights are saved to Google Drive.

"""

class CustomBackbone(nn.Module):
    def __init__(self):
        super(CustomBackbone, self).__init__()
        self.out_channels = 512

        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=3)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)

        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(128)

        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)
        self.bn3 = nn.BatchNorm2d(256)

        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)
        self.bn4 = nn.BatchNorm2d(512)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.relu(self.bn2(self.conv2(x)))
        x = self.relu(self.bn3(self.conv3(x)))
        x = self.relu(self.bn4(self.conv4(x)))

        return x

backbone = CustomBackbone()
backbone_out_channels = 512  # The number of channels in the last layer of the custom backbone.

# Using the built-in AnchorGenerator for RPN
rpn_anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),
                                       aspect_ratios=((0.5, 1.0, 2.0),) * 5)

roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'], output_size=7, sampling_ratio=2)

# Note: Since you're using a custom backbone, the number of out channels might not be the same as ResNet. Ensure that you specify the correct number of out_channels.
model = FasterRCNN(backbone,
                   num_classes=21,
                   rpn_anchor_generator=rpn_anchor_generator,
                   box_roi_pool=roi_pooler,
                   box_detections_per_img=200)

import torch
from google.colab import drive
drive.mount('/content/drive')
from torch.optim.lr_scheduler import StepLR

class Trainer:
    def __init__(self, model, train_loader, test_loader, lr=0.005, momentum=0.9, weight_decay=0.0005, num_epochs=5):
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.test_loader = test_loader
        params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)
        self.scheduler = StepLR(self.optimizer, step_size=30, gamma=0.1)  # Update lr every 30 epochs
        self.num_epochs = num_epochs

    def transform_targets(self, batched_targets):
        transformed_targets = []
        for targets in batched_targets:
            if "boxes" in targets and "labels" in targets:
                # Using the nested dictionary structure
                boxes = torch.tensor(targets['boxes']).float().to(self.device)
                labels = torch.tensor(targets['labels']).long().to(self.device)
                transformed_targets.append({'boxes': boxes, 'labels': labels})
            else:
                # For backward compatibility with the old structure
                boxes_list, labels_list = [], []
                for t in targets:
                    if isinstance(t, str):
                        print("Unexpected string value in targets:", t)
                        continue
                    boxes_list.append([t['xmin'], t['ymin'], t['xmax'], t['ymax']])
                    labels_list.append(t['label'])

                boxes = torch.tensor(boxes_list).float().to(self.device)
                labels = torch.tensor(labels_list).long().to(self.device)
                transformed_targets.append({'boxes': boxes, 'labels': labels})
        return transformed_targets

    def train(self):
        for epoch in range(self.num_epochs):
            self.model.train()
            total_loss = 0.0
            for batch_idx, (batched_images, batched_targets) in enumerate(self.train_loader):
                images = [img.to(self.device) for img in batched_images]
                targets = self.transform_targets(batched_targets)

                loss_dict = self.model(images, targets)
                losses = sum(loss for loss in loss_dict.values())
                total_loss += losses.item()

                self.optimizer.zero_grad()
                losses.backward()
                self.optimizer.step()

                if batch_idx % 10 == 0:
                    print(f"Epoch [{epoch+1}/{self.num_epochs}], Batch [{batch_idx+1}/{len(self.train_loader)}], Loss: {losses.item()}")

            self.scheduler.step()
            avg_loss = total_loss / len(self.train_loader)
            print(f"Epoch [{epoch+1}/{self.num_epochs}], Average Loss: {avg_loss}")

        print("Training Complete!")

    def predict(self):
        self.model.eval()
        with torch.no_grad():
            for images, _ in self.test_loader:
                images = list(img.to(self.device) for img in images)
                prediction = self.model(images)
                print(prediction)
                break

    def save_model(self, path):
        torch.save(self.model.state_dict(), path)
        print(f"Model saved at {path}")

# Usage
trainer = Trainer(model, train_loader, test_loader)
trainer.train()
trainer.predict()
SAVE_PATH = '/content/drive/MyDrive/model_weights_scratch_version_1.pth'
trainer.save_model(SAVE_PATH)

"""### Image data"""

from google.colab import drive
import torch
import cv2
import matplotlib.pyplot as plt

drive.mount('/content/drive')

# Define the inverse label map
inverse_label_map = {v: k for k, v in label_map.items()}

def predict_image(image_path, model, transform, device, threshold=0.1):
    # Read and preprocess the image
    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
    image_tensor = transform(image).unsqueeze(0).to(device)

    # Make predictions
    model.eval()
    with torch.no_grad():
        prediction = model(image_tensor)

    # Visualize predictions
    image_with_boxes = image.copy()
    for box, label, score in zip(prediction[0]['boxes'], prediction[0]['labels'], prediction[0]['scores']):
        if score > threshold:
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (255,0,0), 2)
            cv2.putText(image_with_boxes, inverse_label_map[label.item()], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)

    plt.imshow(image_with_boxes)
    plt.axis('off')
    plt.show()

device = torch.device('cuda')

# Load the trained weights
model_path = '/content/drive/MyDrive/model_weights_scratch_version_1.pth'
model.load_state_dict(torch.load(model_path))
model.to(device)
model.eval()

# Predict for multiple images
image_paths = ['cat_1.jpg', 'cat_2.jpeg', 'dog.jpg', 'dog_6.jpg',
               'cat.jpg', 'dog_3.png', 'dog_4.jpg', 'dog_5.jpg', 'dog_6.jpg', 'person.jpg']
for img_path in image_paths:
    predict_image(img_path, model, transform, device)

"""### Video data"""

from google.colab import drive
import torch
import cv2
import numpy as np
import imageio
from google.colab.patches import cv2_imshow

# Mount the Google Drive
drive.mount('/content/drive')

# Define the inverse label map
inverse_label_map = {v: k for k, v in label_map.items()}

def predict_frame(frame, model, transform, device, threshold=0.1):
    # Convert the frame to RGB
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    frame_tensor = transform(frame_rgb).unsqueeze(0).to(device)

    # Make predictions
    model.eval()
    with torch.no_grad():
        prediction = model(frame_tensor)

    # Visualize predictions on the frame
    frame_with_boxes = frame.copy()
    for box, label, score in zip(prediction[0]['boxes'], prediction[0]['labels'], prediction[0]['scores']):
        if score > threshold:
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(frame_with_boxes, (x1, y1), (x2, y2), (255,0,0), 2)
            cv2.putText(frame_with_boxes, inverse_label_map[label.item()], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)

    return frame_with_boxes

# Set the device
device = torch.device('cuda')

# Load the trained weights
model_path = '/content/drive/MyDrive/model_weights_scratch_version.pth'
model.load_state_dict(torch.load(model_path))
model.to(device)
model.eval()

# Predict for a video and save it
video_path = '/content/video.mp4'
cap = cv2.VideoCapture(video_path)

with imageio.get_writer('/content/output_video.mp4', mode='I', fps=30) as writer:
    while cap.isOpened():
        ret, frame = cap.read()
        if ret:
            # Process the frame and get frame with boxes
            frame_with_boxes = predict_frame(frame, model, transform, device)
            frame_with_boxes_rgb = cv2.cvtColor(frame_with_boxes, cv2.COLOR_BGR2RGB)  # Convert back to RGB before saving
            writer.append_data(frame_with_boxes_rgb)
        else:
            break

cap.release()

print("Video has been saved to /content/output_video.mp4")

"""# Another approach (less customized)"""

import matplotlib.pyplot as plt
import cv2
import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor

# Train and evaluate
from torchvision.models.detection import _utils as det_utils
from torchvision.models.detection.rpn import concat_box_prediction_layers

from google.colab import drive
drive.mount('/content/drive')

# Load a pre-trained model for the COCO dataset
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

# Replace the classifier with a new one for VOC (20 classes + background)
num_classes = 21  # 20 classes + background
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

# This collate function takes care of image sizes being different
def collate_fn(batch):
    return tuple(zip(*batch))

# Loaders
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)

class Trainer:
    def __init__(self, model, train_loader, test_loader, lr=0.005, momentum=0.9, weight_decay=0.0005, num_epochs=1):
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.model = model.to(self.device)
        self.train_loader = train_loader
        self.test_loader = test_loader
        params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)
        self.num_epochs = num_epochs

    def train(self):
        for epoch in range(self.num_epochs):
            self.model.train()
            total_loss = 0.0
            for batch_idx, (batched_images, batched_targets) in enumerate(self.train_loader):
                images = [img.to(self.device) for img in batched_images]
                print(batched_targets)
                targets = [{k: v.to(self.device) for k, v in t.items()} for t in batched_targets]

                loss_dict = self.model(images, targets)
                losses = sum(loss for loss in loss_dict.values())
                total_loss += losses.item()

                self.optimizer.zero_grad()
                losses.backward()
                self.optimizer.step()

                if batch_idx % 10 == 0: # Print every 10 batches. You can change this as per your requirement.
                    print(f"Epoch [{epoch+1}/{self.num_epochs}], Batch [{batch_idx+1}/{len(self.train_loader)}], Loss: {losses.item()}")

            avg_loss = total_loss / len(self.train_loader)
            print(f"Epoch [{epoch+1}/{self.num_epochs}], Average Loss: {avg_loss}")

        print("Training Complete!")

    def predict(self):
        self.model.eval()
        with torch.no_grad():
            for images, _ in self.test_loader:
                images = list(img.to(self.device) for img in images)
                prediction = self.model(images)
                print(prediction)
                break

    def save_model(self, path):
        torch.save(self.model.state_dict(), path)
        print(f"Model saved at {path}")

# Usage
trainer = Trainer(model, train_loader, test_loader, num_epochs=1)
trainer.train()
trainer.predict()
SAVE_PATH = '/content/drive/MyDrive/model_weights_well_pretrained.pth'
trainer.save_model(SAVE_PATH)

inverse_label_map = {v: k for k, v in label_map.items()}

def predict_image(image_path, model, transform, device, threshold=0.6):
    # Read and preprocess the image
    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
    image_tensor = transform(image).unsqueeze(0).to(device)  # Send to device

    # Make predictions
    model.eval()
    with torch.no_grad():
        prediction = model(image_tensor)

    # Visualize predictions
    image_with_boxes = image.copy()
    for box, label, score in zip(prediction[0]['boxes'], prediction[0]['labels'], prediction[0]['scores']):
        if score > threshold:
            # Convert tensor box coordinates to integer values
            x1, y1, x2, y2 = map(int, box)
            cv2.rectangle(image_with_boxes, (x1, y1), (x2, y2), (255,0,0), 2)
            # Use the inverse_label_map to get the class name from the predicted label
            cv2.putText(image_with_boxes, inverse_label_map[label.item()], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,0,0), 2)

    plt.imshow(image_with_boxes)
    plt.axis('off')
    plt.show()

drive.mount('/content/drive')
model_path = '/content/drive/MyDrive/model_weights_well_pretrained.pth'
model.load_state_dict(torch.load(model_path))
model.to(torch.device('cuda'))
model.eval()  # Ensure the model is in evaluation mode

# Usage:
image_path = 'cat_1.jpg'
predict_image(image_path, trainer.model, transform, device)
image_path = 'cat_2.jpeg'
predict_image(image_path, trainer.model, transform, device)
image_path = 'dog.jpg'
predict_image(image_path, trainer.model, transform, device)
image_path = 'dog_6.jpg'
predict_image(image_path, trainer.model, transform, device)